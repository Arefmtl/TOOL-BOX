{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##adding libraris  \n",
    "\n",
    "# import seaborn library for ploting\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pandas.plotting import scatter_matrix\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder #for encoding string data 1hot is better\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data = pd.read_excel(r\"C:\\Users\\ASUS\\Desktop\\code\\karlancer\\DATASETS\\heart.xlsx\",header = 0, names=['name0','name1',...])\n",
    "#housing_shuffled = housing.sample(frac=1, random_state=42)\n",
    "\n",
    "\n",
    "# Load the first CSV dataset\n",
    "#df1 = pd.read_csv(r\"C:\\Users\\ASUS\\Desktop\\code\\karlancer\\clustering\\maincode\\MAIN\\data\\uscities.csv\")\n",
    "\n",
    "# Load the second CSV dataset\n",
    "#df2 = pd.read_csv(r\"C:\\Users\\ASUS\\Desktop\\code\\karlancer\\clustering\\maincode\\MAIN\\data\\encoded_twitter_dataset.csv\")\n",
    "\n",
    "# Concatenate the DataFrames vertically\n",
    "#concatenated_df = pd.concat([df1, df2])\n",
    "\n",
    "# Save the concatenated DataFrame to a new CSV file\n",
    "#concatenated_df.to_csv('concatenated_dataset.csv', index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#data.head(5) #== housing[0:6]\n",
    "\n",
    "#data.info()\n",
    "\n",
    "data.columns\n",
    "#housing['population'].unique() \n",
    "#data['Heart rate'].value_counts()\n",
    "#housing[housing['ocean_proximity']=='ISLAND']\n",
    "#housing[[housing['population'],[housing['ocean_proximity']=='california ']]       \n",
    "#data.describe()\n",
    "#plotting \n",
    "\n",
    "#data.hist(bins =20 , figsize = (20, 15))\n",
    "#plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# train_set && test set\n",
    "\n",
    "train_set, test_set = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "train_set.shape\n",
    "\n",
    "train_set.head()\n",
    "\n",
    "data = train_set.copy()\n",
    "\n",
    "data.head()\n",
    "data.plot(kind = \"scatter\",x=\"Heart rate\", y = 0,s=data[80], label=50,\n",
    "          c=data['Heart rate'], cmap=plt.get_cmap('jet'),figsize=(150,90), alpha=0.2)\n",
    "data.shape\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#standard correlation coefficient[-1, 1] \n",
    "#HAMBASTEGIYE DO MEYAR\n",
    "\n",
    "corr_matrix = data.corr()\n",
    "corr_matrix['Heart rate'].sort_values(ascending=False)\n",
    "\n",
    "\n",
    "#ploting corr\n",
    "features = ['Heart rate', 'Timestamp', 48, 46, 25, 52] \n",
    "\n",
    "scatter_matrix(data[features], figsize=(20, 15))\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "data = train_set.copy()\n",
    "\n",
    "data.head()\n",
    "#data.plot(kind = \"scatter\",x=\"Heart rate\", y = 'Timestamp', figsize=(15, 10), alpha=0.5)\n",
    "#data.shape\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#makong useful data \n",
    "data['total_rooms_per_households'] = data['total_rooms'] / data['households']\n",
    "data['total_bedrooms_per_total_rooms'] = data['total_bedrooms'] / data['total_rooms']\n",
    "data['population_per_households'] = data['population']/data['households']\n",
    "\n",
    "data.head()\n",
    "#ploting new corr plot\n",
    "corr_matrix = data.corr()\n",
    "corr_matrix['median_house_value'].sort_values(ascending=False)\n",
    "features = ['median_house_value', 'median_income', 'total_rooms', 'housing_median_age']\n",
    "scatter_matrix(data[features], figsize=(20, 15))\n",
    "plt.show()\n",
    "\n",
    "# cleaning data\n",
    "\n",
    "\n",
    "df = train_set.copy()\n",
    "df_label = df['Heart rate'].copy()\n",
    "df = df.drop(\"Heart rate\", axis=1)\n",
    "\n",
    "\n",
    "#df_num = df.drop(\"ocean_proximity\", axis=1 )\n",
    "df.info()\n",
    "\n",
    "# Option 1\n",
    "#df_num = df_num.dropna(subset=['total_bedrooms'])\n",
    "\n",
    "# Option 2\n",
    "#f_num = df_num.drop('total_bedrooms', axis=1)\n",
    "#median = df_num['total_rooms'].median()  # Calculate median of 'total_rooms'\n",
    "#f_num['total_rooms'].fillna(median, inplace=True)  # Fill missing values in 'total_rooms' with the calculated median\n",
    "#option 3 #imputing data ~ #option 2\n",
    "#imputer = SimpleImputer(missing_values=np.nan, strategy=\"median\")  # or strategy=\"most_frequent\"\n",
    "#imputer.fit(df_num)\n",
    "#X = imputer.transform(df_num)\n",
    "#df_num_impute_tr = pd.DataFrame(X, columns=df_num.columns)\n",
    "#df_num_impute_tr.info()\n",
    "#df.info()\n",
    "#df_num_impute_tr.head()\n",
    "#add custom attribute to df(useful data)\n",
    "#makong useful data \n",
    "#room_ix, bedroom_ix, population_ix, household_ix = 3, 4, 5, 6\n",
    "\n",
    "#class add_makedup_attribute(BaseEstimator, TransformerMixin):\n",
    "    #def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    #def transform(self, X, y=None):\n",
    "        #rooms_per_household = X[:, room_ix] / X[:, household_ix]\n",
    "        #bedrooms_per_room = X[:, bedroom_ix] / X[:, room_ix]\n",
    "        #population_per_household = X[:, population_ix] / X[:, household_ix]\n",
    "        #return np.c_[X, rooms_per_household, population_per_household, bedrooms_per_room]\n",
    "\n",
    "custom = add_makedup_attribute()\n",
    "data_custom_tr_tmp = custom.transform(df_num_impute_tr.values)\n",
    "data_custom_tr = pd.DataFrame(data_custom_tr_tmp)\n",
    "\n",
    "# Define 'columns' variable\n",
    "columns = df_num.columns.tolist()\n",
    "columns.extend([\"rooms_per_household\", \"population_per_household\", \"bedrooms_per_room\"])\n",
    "\n",
    "data_custom_tr.columns = columns\n",
    "data_custom_tr.head(10)\n",
    "\n",
    "   \n",
    "\n",
    "#feature scaling standardizaion and normalization [0, 1]#(for neural network)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data['Timestamp'] = pd.to_numeric(data['Timestamp'])\n",
    "\n",
    "# فرض می‌گیریم data_custom_tr DataFrame شما حاوی ویژگی‌های عددی است\n",
    "\n",
    "# مقیاس‌دهی به ویژگی‌های عددی\n",
    "feature_scaler = StandardScaler()\n",
    "fdata = pd.DataFrame(feature_scaler.fit_transform(data.values), columns=data.columns)\n",
    "\n",
    "# نمایش DataFrame مقیاس‌داده شده\n",
    "fdata.head()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#labeling encodin or  onehot encoder # text to num algorithm\n",
    "\n",
    "#label encoding problem make relation between encoded nums\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "data_cat = df[\"ocean_proximity\"]\n",
    "data_cat_encoded = encoder.fit_transform(data_cat)\n",
    "data_cat_encoded = pd.DataFrame(data_cat_encoded, columns=['ocean_proximity'])\n",
    "data_cat_encoded.head()\n",
    "\n",
    "#1hot encodinng alternative for last labeling model\n",
    "#if coulmns not so many otherweise lbel encode\n",
    "        \n",
    "\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Initialize OneHotEncoder\n",
    "encoder1hot = OneHotEncoder(sparse=False)\n",
    "\n",
    "# Transform categorical variable 'ocean_proximity' using OneHotEncoder\n",
    "data_cat_1hot_tmp = encoder1hot.fit_transform(df[['ocean_proximity']])\n",
    "\n",
    "# Convert the transformed data into a DataFrame\n",
    "data_cat_1hot = pd.DataFrame(data_cat_1hot_tmp)\n",
    "\n",
    "# Set column names for the one-hot encoded features\n",
    "feature_names_out = encoder1hot.get_feature_names_out(input_features=['ocean_proximity'])\n",
    "data_cat_1hot.columns = feature_names_out\n",
    "\n",
    "# Concatenate the one-hot encoded features with the scaled numerical features\n",
    "final = pd.concat([data_num_scaled_tr, data_cat_1hot], axis=1)\n",
    "\n",
    "# Display the first 10 rows of the final DataFrame\n",
    "final.head(10)\n",
    "\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    ('selector', add_makedup_attribute(num_attrs)),\n",
    "    ('imputer', SimpleImputer(missing_values=np.nan, strategy='median')),\n",
    "    ('attribs_adder', CombinedAttributesAdder()),\n",
    "    ('std_scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "cat_attrs =[\"ocean_proximity\"]\n",
    "cat_pipeline = pipeline([('selector', DataFrameSelector(cat_attrs)),\n",
    "                        ('one_hot_encoder', OneHotEncoder(sparse=False)),\n",
    "                        ])\n",
    "\n",
    "full_pipeline = FeatureUnion(transformer_list=[('num_pipeline', numpipeline),,\n",
    "                                              ('cat_pipeline', catpipeline),\n",
    "                                              ])\n",
    "\n",
    "housing_prepared =full_pipeline.fit_transform(df)\n",
    "housing_prepared_df = pd.DataFrame(housing prepared, cloumns=['longitude', 'latittude', 'housinh_median_age', 'total_rooms',\n",
    "                                                              'total_bedrooms', 'population', 'households', 'median_income',\n",
    "                                           \n",
    "                                                              'rooms_per_household', 'population_per_household', 'bedrooms_per_rooms',\n",
    "                                                              'prox_<1h ocean', 'prox_inland', 'prox_island', 'prox_near bay', 'prox_near ocean'])\n",
    "housing_prepared_df.head(10) \n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(housing_prepared, housing_labels)\n",
    "\n",
    "\n",
    "\n",
    "# Making predictions on a sample of prepared data\n",
    "sample_data_prepared = housing_prepared[:4]\n",
    "predictions = lin_reg.predict(sample_data_prepared)\n",
    "print('Predictions:\\t', predictions)\n",
    "\n",
    "# Actual labels from the sample\n",
    "sample_labels = housing_labels.iloc[:4]\n",
    "print('Labels:\\t\\t', list(sample_labels))\n",
    "\n",
    "# Calculating the accuracy of predictions\n",
    "accuracy = np.mean(np.abs(predictions - sample_labels) / sample_labels)\n",
    "print('Accuracy:\\t', accuracy)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#rms\n",
    "formsklearn.metricks import mean_square_eror\n",
    "housing_prediction = lin_reg.predict(housing_prepared_df)\n",
    "lin_mse = mean_square_eror= lin_reg.predict(housing_prepared_df)\n",
    "lin_rmse = np.sqrt(lin_mse)\n",
    "lin_rmse\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "tree_reg = decisiontreeregressor\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_vall_score(tree_reg, housing_prepared_df, df_label, scoring='neg_mean_squared_error',cv=10, )\n",
    "tree_rmse_scores= np.sqrt(-scores)\n",
    "\n",
    "def display_scores(scores, model_name)\n",
    "\n",
    "    print('==========',model_name,'==========')\n",
    "    print('scores:', scores)\n",
    "    print('mean:', scores.mean())\n",
    "    print('standard')\n",
    "    print('standard_deviation:', score.std())\n",
    "    print('==================================')\n",
    "    \n",
    "    \n",
    "    display_score(trees_rmse_scores, 'deviation tree regression')\n",
    "\n",
    "#linear regresion causes  underfit\n",
    "#and desisin tree causes overfit\n",
    "\n",
    "#random forest the best algoroth cause it test all algorithms zaman bar hast yekam\n",
    "from sklearn.ensemble import random forstregressor\n",
    "forest_reg =randomforestrgressor()\n",
    "forest_reg.fit(housing_prepared_df, df_label)\n",
    "forest_scores = cross_val_score(forest_reg, housing_prepared_df, df_label, scoring='neg_mean_squared_error', cv=10)\n",
    "forest_rmse_scores = np.sqrt(-forest_errors)\n",
    "\n",
    "display_scores(forest_rmse_scores, 'random firest regression ')\n",
    "\n",
    "\n",
    "#  test the best combination of feachres to work on data it takes time ~30 min depend on data\n",
    "\n",
    "from sklearn.ensemble import Random ForestRegressor \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = [['n_estimators': [3, 4, 6, 10, 30], 'max_features': [2, 6, 8, 15]}] forest_reg Random Forest Regressor()\n",
    "\n",
    "grid_search = GridSearchCV(forest_reg, param_grid, cv=5, scoring='neg_mean_squared_error\") grid_search.fit(housing_prepared_df, df_label)\n",
    "\n",
    "print(\"Best Parameters: grid_search.best_params_)\n",
    "\n",
    "print(\"Best Estimator: grid_search.best_estimator_)\n",
    "      \n",
    "      \n",
    "results = grid_search.cv_results_\n",
    "for mean_score, params in zip(results[\"mean_test_score\"], results[\"params\"] print(np.sqrt(-mean_score), params)\n",
    "\n",
    "# Randomized Search => RandomizedSearchcv\n",
    "\n",
    "# Test the final model on the test set\n",
    "\n",
    "final_model = grid_search.best_estimator_\n",
    "\n",
    "X = test_set.drop(\"median_house_value\", axis=1)\n",
    "\n",
    "y = test_set[\"median_house_value\"].copy()\n",
    "\n",
    "X_prepared = full_pipeline.transform(X)\n",
    "\n",
    "final_predictions = final_model.predict(X_prepared)\n",
    "\n",
    "final_mse = mean_squared_error(y, final_predictions)\n",
    "\n",
    "final_rmse = np.sqrt(final_mse)\n",
    "\n",
    "final rmse"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
